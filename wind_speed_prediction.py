# -*- coding: utf-8 -*-
"""Wind_Speed_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TOWyBguaa3D1bDHERp-oN0FWcGplSM0K

## Importing Libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import preprocessing
import scipy.stats as stat

"""# Dataset 
The dataset contains 6574 instances of daily averaged responses from an array of 5 weather variables sensors embedded in a meteorological station. The device was located on the field in a significantly empty area, at 21M. Data were recorded from January 1961 to December 1978 (17 years). Ground Truth daily averaged precipitations, maximum and minimum temperatures, and grass minimum temperature were provided.

Attribute Information

DATE (YYYY-MM-DD)
WIND: Average wind speed (knots)
IND: First indicator value
RAIN: Precipitation Amount (mm)
IND.1: Second indicator value
T.MAX: Maximum Temperature (°C)
IND.2: Third indicator value
T.MIN: Minimum Temperature (°C)
T.MIN.G: 09utc Grass Minimum Temperature (°C)
Data Source: https://www.kaggle.com/datasets/fedesoriano/wind-speed-prediction-dataset

## Data Description
"""

df = pd.read_csv("/content/wind_dataset.csv")
pd.set_option("display.max_columns", None)
df.head(40)

df.info()

df.describe()

print(df.nunique())

"""## EDA/ Data Preproccesing

## Segregating the dataset into various data types

1. Numerical feature
2. Discrete feature
3. Continuous feature
4. Categorical feature
"""

numerical_feature = [feature for feature in df.columns if df[feature].dtypes != 'O']
discrete_feature=[feature for feature in numerical_feature if len(df[feature].unique())<25]
continuous_feature = [feature for feature in numerical_feature if feature not in discrete_feature]
categorical_feature = [feature for feature in df.columns if feature not in numerical_feature]
print("Numerical Features Count {}".format(len(numerical_feature)))
print("Discrete feature Count {}".format(len(discrete_feature)))
print("Continuous feature Count {}".format(len(continuous_feature)))
print("Categorical feature Count {}".format(len(categorical_feature)))

"""## Checking whether the dataset have any missing values"""

# Handle Missing Values
df.isnull().sum()*100/len(df)

"""## As majority of features in the dataset have numerical values.
### So, we are considering it as regreesion problem.
"""

print(numerical_feature)

"""## So, we are using Random sample Imputation

1. Random Sample Imputation take a random observation from the feature.
2. After that we use random observation to replace NaN in that feature.
  It should be used when data is missing completely at random (MCAR)
"""

def randomsampleimputation(df, variable):
    df[variable]=df[variable]
    random_sample=df[variable].dropna().sample(df[variable].isnull().sum(),random_state=0)
    random_sample.index=df[df[variable].isnull()].index
    df.loc[df[variable].isnull(),variable]=random_sample

"""## We are implemeting RandomSampleImputation for 5 of the continous features as it has NaN values"""

randomsampleimputation(df, "IND.1")
randomsampleimputation(df, "T.MAX")
randomsampleimputation(df, "IND.2")
randomsampleimputation(df, "T.MIN")
randomsampleimputation(df, "T.MIN.G")

df.head(12)

"""## Correlation matrix before handling missing values"""

corrmat = df.corr(method = "spearman")
plt.figure(figsize=(20,20))
#plot heat map
g=sns.heatmap(corrmat,annot=True)

"""## Seaborn Distplot

### distplot plots a univariate distribution of observations and  shows a histogram with a line on it.
### This can be used shown for all kinds of variations
"""

for feature in continuous_feature:
    data=df.copy()
    sns.distplot(df[feature])
    plt.xlabel(feature)
    plt.ylabel("Count")
    plt.title(feature)
    plt.figure(figsize=(15,15))
    plt.show()

"""## Box Plot
### A for loop is used to plot a boxplot for all the continuous features to see the outliers

"""

for feature in continuous_feature:
    data=df.copy()
    sns.boxplot(data[feature])
    plt.title(feature)
    plt.figure(figsize=(15,15))

"""## Impute missing values with Median:
Columns in the dataset which are having numeric continuous values can be replaced with the mean, median, or mode of remaining values in the column. This method can prevent the loss of data compared to the earlier method. Replacing the approximations (median) is a statistical approach to handle the missing values.
"""

print(continuous_feature)

for feature in continuous_feature:
    if(df[feature].isnull().sum()*100/len(df))>0:
        df[feature] = df[feature].fillna(df[feature].median())

df.isnull().sum()*100/len(df)

discrete_feature

def mode_nan(df,variable):
    mode=df[variable].value_counts().index[0]
    df[variable].fillna(mode,inplace=True)
mode_nan(df,"IND")
mode_nan(df,"IND.1")
mode_nan(df,"IND.2")

"""## We need to convert the DATE dtype from object to a datetime"""

df["DATE"] = pd.to_datetime(df["DATE"], format = "%Y-%m-%dT", errors = "coerce")

df["Date_year"] = df["DATE"].dt.year
df["Date_month"] = df["DATE"].dt.month
df["Date_day"] = df["DATE"].dt.day

df

"""## Correlation matrix after handling missing values"""

corrmat = df.corr()
plt.figure(figsize=(20,20))
#plot heat map
g=sns.heatmap(corrmat,annot=True)

"""## Box Plot"""

for feature in continuous_feature:
    data=df.copy()
    sns.boxplot(data[feature])
    plt.title(feature)
    plt.figure(figsize=(15,15))

"""## qqplot (Quantile-Quantile Plot)
When the quantiles of two variables are plotted against each other, then the plot obtained is known as quantile – quantile plot or qqplot. This plot provides a summary of whether the distributions of two variables are similar or not with respect to the locations.

### All point of quantiles lie on or close to straight line at an angle of 45 degree from x – axis. It indicates that two samples have similar distributions. But in practical it is not possible. Anyhow, the points are lying nearly on the straight line are considerd to be a good model.
"""

def qq_plots(df, variable):
    plt.figure(figsize=(15,6))
    plt.subplot(1, 2, 1)
    df[variable].hist()
    plt.subplot(1, 2, 2)
    stats.probplot(df[variable], dist="norm", plot=plt)
    plt.show()

import scipy.stats as stats

for feature in continuous_feature:
    print(feature)
    plt.figure(figsize=(15,6))
    plt.subplot(1, 2, 1)
    df[feature].hist()
    plt.subplot(1, 2, 2)
    stats.probplot(df[feature], dist="norm", plot=plt)
    plt.show()

"""## Cleaned and Saving the preprocessed model to .csv file"""

df.to_csv("preprocessed_wind_data.csv", index=False)

"""## Dropping unwanted columns"""

X = df.drop(["WIND", "DATE"], axis=1)
Y = df["WIND"]

"""### Splitting the data into training and test set"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=123)

y_train

print('Training dataset shape:', X_train.shape, y_train.shape)
print('Testing dataset shape:', X_test.shape, y_test.shape)

import joblib

"""# Linear Regression

"""

from sklearn.linear_model import LinearRegression
from sklearn import metrics

lin_reg = LinearRegression()
lin_reg.fit(X_train, y_train)
lin_pred = lin_reg.predict(X_test)

print('MAE:', metrics.mean_absolute_error(y_test, lin_pred))
print('MSE:', metrics.mean_squared_error(y_test, lin_pred))
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, lin_pred)))

"""# Decision Tree

"""

from sklearn.tree import DecisionTreeRegressor

dtr = DecisionTreeRegressor()
dtr.fit(X_train, y_train)
dtr_pred = dtr.predict(X_test)

print('MAE:', metrics.mean_absolute_error(y_test, dtr_pred))
print('MSE:', metrics.mean_squared_error(y_test, dtr_pred))
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, dtr_pred)))

"""# Random Forest Regressor"""

from sklearn.ensemble import RandomForestRegressor

rfr = RandomForestRegressor(max_depth=2, random_state=0)
rfr.fit(X_train, y_train)
rfr_pred = rfr.predict(X_test)

print('MAE:', metrics.mean_absolute_error(y_test, rfr_pred))
print('MSE:', metrics.mean_squared_error(y_test, rfr_pred))
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, rfr_pred)))

"""# Support vector Regressor"""

from sklearn.svm import SVR
svr = SVR(kernel='rbf', C=1, epsilon=10)
svr.fit(X_train,y_train)
svr_pred = svr.predict(X_test)

print('MAE:', metrics.mean_absolute_error(y_test, svr_pred))
print('MSE:', metrics.mean_squared_error(y_test, svr_pred))
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, svr_pred)))

"""# Light GBM"""

!pip install lightgbm

import lightgbm as lgb

hyper_params = {
    'task': 'train',
    'boosting_type': 'gbdt',
    'objective': 'regression',
    'metric': ['l2', 'auc'],
    'learning_rate': 0.005,
    "num_leaves": 128,  
    "max_bin": 512,
}

rg = lgb.LGBMRegressor(**hyper_params)

rg.fit(X_train,y_train)

rg_pred = rg.predict(X_test)

print('MAE:', metrics.mean_absolute_error(y_test, rg_pred))
print('MSE:', metrics.mean_squared_error(y_test, rg_pred))
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, rg_pred)))

"""## Tabulating the output values from the model"""

from tabulate import tabulate

all_data = [["Regression Models","MAE","MSE","RMSE"],
            ["Light GBM",3.73, 21.74, 4.66],
            ["Random Forest Regressor",3.75,22.25,4.72],
            ["Linear Regression",3.87,23.38,4.84],
            ["Support vector Regressor",4.55,29.56,5.44],
            ["Decision Tree Regressor",4.79,38.10,6.17]]

print(tabulate(all_data,headers='firstrow',tablefmt='fancy_grid'))

